#!/usr/bin/env python3
"""
NoxSuite AI/ML Integration Validation Script
===========================================

This script validates that all AI/ML components are working correctly.
"""

import sys
import os
import traceback
from datetime import datetime

def test_basic_imports():
    """Test that all required libraries can be imported."""
    print("üîç Testing basic imports...")
    
    try:
        import numpy as np
        print("  ‚úÖ NumPy available")
    except ImportError as e:
        print(f"  ‚ùå NumPy import failed: {e}")
        return False
    
    try:
        import pandas as pd
        print("  ‚úÖ Pandas available")
    except ImportError as e:
        print(f"  ‚ùå Pandas import failed: {e}")
        return False
    
    try:
        from sklearn.ensemble import IsolationForest
        print("  ‚úÖ Scikit-learn available")
    except ImportError as e:
        print(f"  ‚ùå Scikit-learn import failed: {e}")
        return False
    
    # Test optional libraries
    try:
        import tensorflow as tf
        print("  ‚úÖ TensorFlow available")
    except ImportError:
        print("  ‚ö†Ô∏è  TensorFlow not available (optional)")
    
    try:
        import torch
        print("  ‚úÖ PyTorch available")
    except ImportError:
        print("  ‚ö†Ô∏è  PyTorch not available (optional)")
    
    try:
        import xgboost as xgb
        print("  ‚úÖ XGBoost available")
    except ImportError:
        print("  ‚ö†Ô∏è  XGBoost not available (optional)")
    
    try:
        import lightgbm as lgb
        print("  ‚úÖ LightGBM available")
    except ImportError:
        print("  ‚ö†Ô∏è  LightGBM not available (optional)")
    
    try:
        import nltk
        print("  ‚úÖ NLTK available")
    except ImportError:
        print("  ‚ö†Ô∏è  NLTK not available (optional)")
    
    try:
        import cv2
        print("  ‚úÖ OpenCV available")
    except ImportError:
        print("  ‚ö†Ô∏è  OpenCV not available (optional)")
    
    return True


def test_ml_modules():
    """Test that ML modules can be imported and basic functionality works."""
    print("\nüîç Testing ML modules...")
    
    try:
        from ml.model_training import SecurityModelTrainer, create_sample_security_data
        print("  ‚úÖ Model training module imported")
        
        # Test basic functionality
        trainer = SecurityModelTrainer(model_dir="/tmp/test_models")
        sample_data = create_sample_security_data(50)
        print(f"  ‚úÖ Created sample data: {len(sample_data)} records")
        
    except Exception as e:
        print(f"  ‚ùå Model training module failed: {e}")
        traceback.print_exc()
        return False
    
    try:
        from ml.anomaly_detection import AnomalyDetector, create_sample_system_metrics
        print("  ‚úÖ Anomaly detection module imported")
        
        # Test basic functionality
        detector = AnomalyDetector()
        metrics = create_sample_system_metrics()
        print(f"  ‚úÖ Created sample metrics: {list(metrics.keys())}")
        
    except Exception as e:
        print(f"  ‚ùå Anomaly detection module failed: {e}")
        traceback.print_exc()
        return False
    
    try:
        from ml.predictive_engine import RiskScoreEngine, create_sample_training_data
        print("  ‚úÖ Risk scoring module imported")
        
        # Test basic functionality
        engine = RiskScoreEngine(model_dir="/tmp/test_models")
        training_data = create_sample_training_data(50)
        print(f"  ‚úÖ Created sample training data: {len(training_data)} records")
        
    except Exception as e:
        print(f"  ‚ùå Risk scoring module failed: {e}")
        traceback.print_exc()
        return False
    
    return True


def test_model_training():
    """Test model training functionality."""
    print("\nüîç Testing model training...")
    
    try:
        from ml.model_training import SecurityModelTrainer, create_sample_security_data
        
        trainer = SecurityModelTrainer(model_dir="/tmp/test_models")
        sample_data = create_sample_security_data(100)
        
        # Test anomaly detection training
        results = trainer.train_anomaly_detector(sample_data, "test_anomaly")
        print(f"  ‚úÖ Anomaly model trained - anomaly ratio: {results['anomaly_ratio']:.3f}")
        
        # Test classification training
        results = trainer.train_classification_model(sample_data, "test_classifier")
        print(f"  ‚úÖ Classification model trained - accuracy: {results['accuracy']:.3f}")
        
        # Test model listing
        models = trainer.list_models()
        print(f"  ‚úÖ Models available: {models}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Model training failed: {e}")
        traceback.print_exc()
        return False


def test_anomaly_detection():
    """Test anomaly detection functionality."""
    print("\nüîç Testing anomaly detection...")
    
    try:
        from ml.anomaly_detection import AnomalyDetector, create_sample_system_metrics
        import pandas as pd
        
        detector = AnomalyDetector()
        
        # Test system metrics detection
        metrics = create_sample_system_metrics()
        alerts = detector.detect_system_metric_anomalies(metrics)
        print(f"  ‚úÖ System metrics analyzed - {len(alerts)} alerts generated")
        
        # Test login anomaly detection with sample data
        login_data = pd.DataFrame({
            'timestamp': pd.date_range('2024-01-01', periods=150, freq='H'),
            'user_id': ['user1'] * 150,
            'ip_address': ['192.168.1.10'] * 150,
            'status': ['success'] * 150
        })
        
        if len(login_data) >= detector.config['min_samples_for_training']:
            login_alerts = detector.detect_login_anomalies(login_data)
            print(f"  ‚úÖ Login patterns analyzed - {len(login_alerts)} alerts generated")
        else:
            print(f"  ‚ö†Ô∏è  Skipped login analysis - insufficient data")
        
        # Test alert summary
        summary = detector.get_alert_summary()
        print(f"  ‚úÖ Alert summary generated - {summary['total_alerts']} total alerts")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Anomaly detection failed: {e}")
        traceback.print_exc()
        return False


def test_risk_scoring():
    """Test risk scoring functionality."""
    print("\nüîç Testing risk scoring...")
    
    try:
        from ml.predictive_engine import RiskScoreEngine, create_sample_training_data
        import pandas as pd
        
        engine = RiskScoreEngine(model_dir="/tmp/test_models")
        
        # Test user risk scoring
        user_data = pd.DataFrame({
            'timestamp': pd.date_range('2024-01-01', periods=50, freq='H'),
            'user_id': ['test_user'] * 50,
            'ip_address': ['192.168.1.10'] * 30 + ['203.0.113.1'] * 20,
            'status': ['success'] * 40 + ['failed'] * 10
        })
        
        user_risk = engine.calculate_user_risk_score('test_user', user_data)
        print(f"  ‚úÖ User risk calculated - Score: {user_risk.risk_score:.3f}, Level: {user_risk.risk_level.value}")
        
        # Test session risk scoring
        session_data = {
            'duration_minutes': 30,
            'pages_accessed': 5,
            'failed_attempts': 0,
            'ip_address': '192.168.1.100'
        }
        
        session_risk = engine.calculate_session_risk_score('test_session', session_data)
        print(f"  ‚úÖ Session risk calculated - Score: {session_risk.risk_score:.3f}, Level: {session_risk.risk_level.value}")
        
        # Test IP risk scoring
        ip_risk = engine.calculate_ip_risk_score('203.0.113.1', user_data)
        print(f"  ‚úÖ IP risk calculated - Score: {ip_risk.risk_score:.3f}, Level: {ip_risk.risk_level.value}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Risk scoring failed: {e}")
        traceback.print_exc()
        return False


def test_model_persistence():
    """Test model saving and loading."""
    print("\nüîç Testing model persistence...")
    
    try:
        from ml.model_training import SecurityModelTrainer, create_sample_security_data
        
        # Train and save a model
        trainer1 = SecurityModelTrainer(model_dir="/tmp/test_models")
        sample_data = create_sample_security_data(100)
        trainer1.train_anomaly_detector(sample_data, "persistence_test")
        
        # Load model in new trainer instance
        trainer2 = SecurityModelTrainer(model_dir="/tmp/test_models")
        loaded_model = trainer2.load_model("persistence_test")
        
        print("  ‚úÖ Model saved and loaded successfully")
        
        # Verify model works
        features, _ = trainer2.prepare_security_data(sample_data.head(10))
        if 'persistence_test_scaler' in trainer1.scalers:
            features_scaled = trainer1.scalers['persistence_test_scaler'].transform(features)
            predictions = loaded_model.predict(features_scaled)
            print(f"  ‚úÖ Loaded model made predictions: {len(predictions)} outputs")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Model persistence failed: {e}")
        traceback.print_exc()
        return False


def run_comprehensive_test():
    """Run comprehensive ML integration test."""
    print("\nüîç Running comprehensive integration test...")
    
    try:
        from ml.model_training import SecurityModelTrainer, create_sample_security_data
        from ml.anomaly_detection import AnomalyDetector
        from ml.predictive_engine import RiskScoreEngine, create_sample_training_data
        import pandas as pd
        
        # 1. Create components
        trainer = SecurityModelTrainer(model_dir="/tmp/test_models")
        detector = AnomalyDetector()
        engine = RiskScoreEngine(model_dir="/tmp/test_models")
        
        # 2. Generate test data
        security_logs = create_sample_security_data(200)
        training_data = create_sample_training_data(200)
        
        # 3. Train models
        trainer.train_anomaly_detector(security_logs.head(100), "integration_anomaly")
        trainer.train_classification_model(security_logs.head(100), "integration_classifier")
        
        if len(training_data) >= engine.config['min_training_samples']:
            engine.train_risk_models(training_data)
            print("  ‚úÖ Risk models trained")
        
        # 4. Test real-time detection
        test_metrics = {'cpu_usage': 95.0, 'memory_usage': 90.0}
        alerts = detector.detect_system_metric_anomalies(test_metrics)
        
        # 5. Test risk scoring
        user_data = security_logs[security_logs['user_id'] == security_logs['user_id'].iloc[0]].head(20)
        risk_score = engine.calculate_user_risk_score('integration_test_user', user_data)
        
        print(f"  ‚úÖ Integration test completed successfully")
        print(f"      - Models trained: {len(trainer.list_models())}")
        print(f"      - Alerts generated: {len(alerts)}")
        print(f"      - Risk score: {risk_score.risk_score:.3f}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Integration test failed: {e}")
        traceback.print_exc()
        return False


def main():
    """Main validation function."""
    print("ü§ñ NoxSuite AI/ML Integration Validation")
    print("=" * 50)
    print(f"Timestamp: {datetime.now()}")
    print(f"Python: {sys.version}")
    print(f"Working Directory: {os.getcwd()}")
    print()
    
    tests = [
        ("Basic Imports", test_basic_imports),
        ("ML Modules", test_ml_modules),
        ("Model Training", test_model_training),
        ("Anomaly Detection", test_anomaly_detection),
        ("Risk Scoring", test_risk_scoring),
        ("Model Persistence", test_model_persistence),
        ("Comprehensive Integration", run_comprehensive_test)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\n{'=' * 20} {test_name} {'=' * 20}")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 50)
    print("üìä VALIDATION SUMMARY")
    print("=" * 50)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, passed_test in results.items():
        status = "‚úÖ PASS" if passed_test else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    print(f"\nOverall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ All tests passed! AI/ML integration is working correctly.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed. Please check the output above.")
        return 1


if __name__ == "__main__":
    sys.exit(main())