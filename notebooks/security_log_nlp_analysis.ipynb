{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Security Log Analysis with NLP\n",
    "\n",
    "This notebook demonstrates Natural Language Processing capabilities for parsing and analyzing security logs using NLTK and other NLP libraries.\n",
    "\n",
    "## Features:\n",
    "- Log message parsing and classification\n",
    "- Threat intelligence extraction\n",
    "- Anomaly detection in log patterns\n",
    "- Automated incident categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    from nltk.chunk import ne_chunk\n",
    "    from nltk.tag import pos_tag\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('maxent_ne_chunker', quiet=True)\n",
    "    nltk.download('words', quiet=True)\n",
    "    \n",
    "    print(\"✅ NLTK libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing NLTK: {e}\")\n",
    "    print(\"Please install NLTK: pip install nltk\")\n",
    "\n",
    "# Machine Learning libraries\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"✅ Scikit-learn libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing scikit-learn: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sample Security Log Data\n",
    "\n",
    "Create sample security log data with various types of security events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample security log messages\n",
    "sample_logs = [\n",
    "    \"Failed login attempt from IP 203.0.113.1 for user admin at 2024-01-15 03:45:22\",\n",
    "    \"Successful authentication for user john.doe from 192.168.1.100\",\n",
    "    \"Multiple failed login attempts detected from IP 198.51.100.50 - possible brute force attack\",\n",
    "    \"File access denied: user guest attempted to access /etc/passwd\",\n",
    "    \"Privilege escalation attempt: user temp_user tried to execute sudo command\",\n",
    "    \"Unusual network traffic detected: large data transfer to external IP 203.0.113.200\",\n",
    "    \"Malware signature detected in file upload: trojan.exe blocked\",\n",
    "    \"SQL injection attempt blocked on login form from IP 198.51.100.75\",\n",
    "    \"Firewall rule violation: blocked connection from 203.0.113.150 on port 22\",\n",
    "    \"Successful user logout: session terminated for user admin\",\n",
    "    \"Account lockout: user account 'testuser' locked due to multiple failed attempts\",\n",
    "    \"Configuration change: firewall rule modified by administrator\",\n",
    "    \"Certificate expired: SSL certificate for domain.com needs renewal\",\n",
    "    \"Port scan detected: sequential port access from IP 198.51.100.100\",\n",
    "    \"Virus quarantined: infected file detected and isolated\",\n",
    "    \"Data exfiltration alert: unusual outbound traffic pattern detected\",\n",
    "    \"Phishing attempt: suspicious email link clicked by user employee1\",\n",
    "    \"Password policy violation: weak password detected for user newuser\",\n",
    "    \"Time synchronization error: system clock drift detected\",\n",
    "    \"Memory usage alert: high memory consumption on server01\",\n",
    "    \"Disk space warning: storage 90% full on database server\",\n",
    "    \"Service restart: web server automatically restarted after crash\",\n",
    "    \"Backup completed successfully: daily backup finished at 02:00\",\n",
    "    \"VPN connection established: remote user connected from 203.0.113.75\",\n",
    "    \"Cross-site scripting attempt blocked on contact form\"\n",
    "]\n",
    "\n",
    "# Create corresponding categories\n",
    "log_categories = [\n",
    "    \"authentication\", \"authentication\", \"authentication\", \"access_control\",\n",
    "    \"privilege_escalation\", \"network_anomaly\", \"malware\", \"injection_attack\",\n",
    "    \"firewall\", \"authentication\", \"account_management\", \"configuration\",\n",
    "    \"certificate\", \"network_scan\", \"malware\", \"data_exfiltration\",\n",
    "    \"phishing\", \"password_policy\", \"system_error\", \"system_alert\",\n",
    "    \"system_alert\", \"system_error\", \"backup\", \"vpn\", \"injection_attack\"\n",
    "]\n",
    "\n",
    "# Create severity levels\n",
    "log_severity = [\n",
    "    \"medium\", \"low\", \"high\", \"medium\", \"high\", \"high\", \"critical\", \"high\",\n",
    "    \"medium\", \"low\", \"medium\", \"low\", \"medium\", \"high\", \"critical\",\n",
    "    \"critical\", \"high\", \"medium\", \"low\", \"medium\", \"medium\", \"low\",\n",
    "    \"low\", \"low\", \"high\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "logs_df = pd.DataFrame({\n",
    "    'timestamp': pd.date_range('2024-01-15', periods=len(sample_logs), freq='H'),\n",
    "    'message': sample_logs,\n",
    "    'category': log_categories,\n",
    "    'severity': log_severity\n",
    "})\n",
    "\n",
    "print(f\"Created {len(logs_df)} sample log entries\")\n",
    "print(\"\\nSample log entries:\")\n",
    "logs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Log Message Preprocessing\n",
    "\n",
    "Clean and preprocess log messages for NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def preprocess_log_message(message):\n",
    "    \"\"\"Preprocess log message for NLP analysis.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    message = message.lower()\n",
    "    \n",
    "    # Remove timestamps and IP addresses for cleaner text analysis\n",
    "    message = re.sub(r'\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}', '', message)\n",
    "    message = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', 'IP_ADDRESS', message)\n",
    "    \n",
    "    # Remove common file paths\n",
    "    message = re.sub(r'/[\\w/]+', 'FILE_PATH', message)\n",
    "    \n",
    "    # Remove special characters but keep alphanumeric and spaces\n",
    "    message = re.sub(r'[^a-zA-Z0-9\\s]', ' ', message)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    message = ' '.join(message.split())\n",
    "    \n",
    "    return message\n",
    "\n",
    "def extract_entities(message):\n",
    "    \"\"\"Extract important entities from log messages.\"\"\"\n",
    "    entities = {\n",
    "        'ip_addresses': re.findall(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', message),\n",
    "        'usernames': re.findall(r'user\\s+(\\w+)', message),\n",
    "        'ports': re.findall(r'port\\s+(\\d+)', message),\n",
    "        'files': re.findall(r'([\\w\\.]+\\.(exe|pdf|doc|txt|php|sql))', message)\n",
    "    }\n",
    "    return entities\n",
    "\n",
    "# Apply preprocessing\n",
    "logs_df['processed_message'] = logs_df['message'].apply(preprocess_log_message)\n",
    "logs_df['entities'] = logs_df['message'].apply(extract_entities)\n",
    "\n",
    "print(\"Log preprocessing completed\")\n",
    "print(\"\\nExample of preprocessed messages:\")\n",
    "for i in range(5):\n",
    "    print(f\"Original: {logs_df.iloc[i]['message']}\")\n",
    "    print(f\"Processed: {logs_df.iloc[i]['processed_message']}\")\n",
    "    print(f\"Entities: {logs_df.iloc[i]['entities']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Log Categorization using NLP\n",
    "\n",
    "Use machine learning to automatically categorize security log messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and feature extraction\n",
    "def tokenize_and_clean(text):\n",
    "    \"\"\"Tokenize text and remove stopwords.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply tokenization\n",
    "logs_df['tokenized_message'] = logs_df['processed_message'].apply(tokenize_and_clean)\n",
    "\n",
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "X_features = vectorizer.fit_transform(logs_df['tokenized_message'])\n",
    "\n",
    "print(f\"Created TF-IDF features: {X_features.shape}\")\n",
    "print(f\"Feature names (first 20): {vectorizer.get_feature_names_out()[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train log classification model\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, logs_df['category'], test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Log Classification Results:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Show feature importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_features_per_category = {}\n",
    "\n",
    "for category in classifier.classes_:\n",
    "    category_idx = list(classifier.classes_).index(category)\n",
    "    top_features_idx = classifier.feature_log_prob_[category_idx].argsort()[-10:][::-1]\n",
    "    top_features_per_category[category] = [feature_names[i] for i in top_features_idx]\n",
    "\n",
    "print(\"\\nTop features per category:\")\n",
    "for category, features in top_features_per_category.items():\n",
    "    print(f\"{category}: {', '.join(features[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling for Log Analysis\n",
    "\n",
    "Use Latent Dirichlet Allocation (LDA) to discover hidden topics in security logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform topic modeling using LDA\n",
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "\n",
    "# Fit LDA model\n",
    "lda.fit(X_features)\n",
    "\n",
    "# Get topic-word distributions\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-no_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "topics = display_topics(lda, feature_names, 10)\n",
    "\n",
    "print(\"Discovered Topics:\")\n",
    "for topic_name, words in topics.items():\n",
    "    print(f\"{topic_name}: {', '.join(words)}\")\n",
    "\n",
    "# Get topic distributions for each log\n",
    "doc_topic_dist = lda.transform(X_features)\n",
    "logs_df['dominant_topic'] = doc_topic_dist.argmax(axis=1) + 1\n",
    "logs_df['topic_confidence'] = doc_topic_dist.max(axis=1)\n",
    "\n",
    "print(\"\\nSample logs with their dominant topics:\")\n",
    "sample_logs_with_topics = logs_df[['message', 'category', 'dominant_topic', 'topic_confidence']].head(10)\n",
    "sample_logs_with_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threat Intelligence Extraction\n",
    "\n",
    "Extract and analyze threat indicators from security logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract threat indicators\n",
    "def extract_threat_indicators(logs_df):\n",
    "    \"\"\"Extract various threat indicators from logs.\"\"\"\n",
    "    \n",
    "    indicators = {\n",
    "        'suspicious_ips': set(),\n",
    "        'malicious_files': set(),\n",
    "        'attack_types': Counter(),\n",
    "        'targeted_users': Counter(),\n",
    "        'attack_patterns': Counter()\n",
    "    }\n",
    "    \n",
    "    for _, log in logs_df.iterrows():\n",
    "        message = log['message'].lower()\n",
    "        \n",
    "        # Extract suspicious IPs (external IPs in security events)\n",
    "        if log['severity'] in ['high', 'critical']:\n",
    "            ips = log['entities']['ip_addresses']\n",
    "            for ip in ips:\n",
    "                if not ip.startswith(('192.168.', '10.', '172.16.')):\n",
    "                    indicators['suspicious_ips'].add(ip)\n",
    "        \n",
    "        # Extract malicious files\n",
    "        if 'malware' in message or 'virus' in message or 'trojan' in message:\n",
    "            files = log['entities']['files']\n",
    "            indicators['malicious_files'].update(files)\n",
    "        \n",
    "        # Count attack types\n",
    "        attack_keywords = {\n",
    "            'brute_force': ['brute force', 'multiple failed', 'failed attempt'],\n",
    "            'injection': ['sql injection', 'script injection', 'xss'],\n",
    "            'malware': ['malware', 'virus', 'trojan'],\n",
    "            'privilege_escalation': ['privilege escalation', 'sudo'],\n",
    "            'data_exfiltration': ['data exfiltration', 'unusual outbound'],\n",
    "            'port_scan': ['port scan', 'sequential port']\n",
    "        }\n",
    "        \n",
    "        for attack_type, keywords in attack_keywords.items():\n",
    "            if any(keyword in message for keyword in keywords):\n",
    "                indicators['attack_types'][attack_type] += 1\n",
    "        \n",
    "        # Extract targeted users\n",
    "        users = log['entities']['usernames']\n",
    "        if log['severity'] in ['medium', 'high', 'critical']:\n",
    "            indicators['targeted_users'].update(users)\n",
    "        \n",
    "        # Identify attack patterns\n",
    "        if 'failed' in message:\n",
    "            indicators['attack_patterns']['authentication_failure'] += 1\n",
    "        if 'blocked' in message:\n",
    "            indicators['attack_patterns']['blocked_access'] += 1\n",
    "        if 'detected' in message:\n",
    "            indicators['attack_patterns']['detected_threat'] += 1\n",
    "    \n",
    "    return indicators\n",
    "\n",
    "# Extract threat indicators\n",
    "threat_indicators = extract_threat_indicators(logs_df)\n",
    "\n",
    "print(\"Threat Intelligence Summary:\")\n",
    "print(f\"\\nSuspicious IPs: {list(threat_indicators['suspicious_ips'])}\")\n",
    "print(f\"Malicious Files: {list(threat_indicators['malicious_files'])}\")\n",
    "print(f\"\\nAttack Types:\")\n",
    "for attack_type, count in threat_indicators['attack_types'].most_common():\n",
    "    print(f\"  {attack_type}: {count}\")\n",
    "    \n",
    "print(f\"\\nTargeted Users:\")\n",
    "for user, count in threat_indicators['targeted_users'].most_common():\n",
    "    print(f\"  {user}: {count}\")\n",
    "    \n",
    "print(f\"\\nAttack Patterns:\")\n",
    "for pattern, count in threat_indicators['attack_patterns'].most_common():\n",
    "    print(f\"  {pattern}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Clustering for Anomaly Detection\n",
    "\n",
    "Use clustering to identify unusual log patterns that might indicate new threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on log messages\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_features.toarray())\n",
    "\n",
    "logs_df['cluster'] = cluster_labels\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"Log Cluster Analysis:\")\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_logs = logs_df[logs_df['cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_logs)} logs):\")\n",
    "    \n",
    "    # Most common categories in this cluster\n",
    "    common_categories = cluster_logs['category'].value_counts().head(3)\n",
    "    print(f\"  Common categories: {dict(common_categories)}\")\n",
    "    \n",
    "    # Severity distribution\n",
    "    severity_dist = cluster_logs['severity'].value_counts()\n",
    "    print(f\"  Severity distribution: {dict(severity_dist)}\")\n",
    "    \n",
    "    # Sample messages\n",
    "    print(f\"  Sample messages:\")\n",
    "    for msg in cluster_logs['message'].head(2):\n",
    "        print(f\"    - {msg}\")\n",
    "\n",
    "# Identify potential anomalies (small clusters with high severity)\n",
    "cluster_analysis = logs_df.groupby('cluster').agg({\n",
    "    'message': 'count',\n",
    "    'severity': lambda x: (x == 'critical').sum() + (x == 'high').sum()\n",
    "}).rename(columns={'message': 'log_count', 'severity': 'high_severity_count'})\n",
    "\n",
    "cluster_analysis['anomaly_score'] = (\n",
    "    cluster_analysis['high_severity_count'] / cluster_analysis['log_count']\n",
    ") * (1 / (cluster_analysis['log_count'] + 1))  # Smaller clusters get higher scores\n",
    "\n",
    "print(\"\\nPotential Anomalous Clusters (sorted by anomaly score):\")\n",
    "anomalous_clusters = cluster_analysis.sort_values('anomaly_score', ascending=False)\n",
    "print(anomalous_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization and Reporting\n",
    "\n",
    "Create visualizations to summarize the NLP analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "# 1. Log category distribution\n",
    "logs_df['category'].value_counts().plot(kind='bar', ax=axes[0,0], title='Log Category Distribution')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# 2. Severity distribution\n",
    "severity_order = ['low', 'medium', 'high', 'critical']\n",
    "logs_df['severity'].value_counts().reindex(severity_order).plot(\n",
    "    kind='bar', ax=axes[0,1], title='Severity Distribution', color='orange'\n",
    ")\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].set_ylabel('Count')\n",
    "\n",
    "# 3. Attack types distribution\n",
    "attack_types = dict(threat_indicators['attack_types'])\n",
    "if attack_types:\n",
    "    pd.Series(attack_types).plot(kind='bar', ax=axes[1,0], title='Attack Types', color='red')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "\n",
    "# 4. Cluster distribution\n",
    "logs_df['cluster'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[1,1], title='Log Clusters', color='green'\n",
    ")\n",
    "axes[1,1].set_xlabel('Cluster ID')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "\n",
    "# 5. Topic distribution\n",
    "logs_df['dominant_topic'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[2,0], title='Dominant Topics', color='purple'\n",
    ")\n",
    "axes[2,0].set_xlabel('Topic ID')\n",
    "axes[2,0].set_ylabel('Count')\n",
    "\n",
    "# 6. Timeline of severity\n",
    "severity_timeline = logs_df.set_index('timestamp')['severity']\n",
    "severity_counts_timeline = severity_timeline.groupby([severity_timeline.index.date, severity_timeline]).size().unstack(fill_value=0)\n",
    "if not severity_counts_timeline.empty:\n",
    "    severity_counts_timeline.plot(kind='bar', stacked=True, ax=axes[2,1], title='Severity Timeline')\n",
    "    axes[2,1].tick_params(axis='x', rotation=45)\n",
    "    axes[2,1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate summary report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SECURITY LOG ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAnalysis Period: {logs_df['timestamp'].min()} to {logs_df['timestamp'].max()}\")\n",
    "print(f\"Total Log Entries Analyzed: {len(logs_df)}\")\n",
    "\n",
    "print(f\"\\nSeverity Breakdown:\")\n",
    "for severity in severity_order:\n",
    "    count = (logs_df['severity'] == severity).sum()\n",
    "    percentage = (count / len(logs_df)) * 100\n",
    "    print(f\"  {severity.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTop Security Categories:\")\n",
    "for category, count in logs_df['category'].value_counts().head(5).items():\n",
    "    print(f\"  {category}: {count}\")\n",
    "\n",
    "print(f\"\\nThreat Intelligence Highlights:\")\n",
    "print(f\"  Suspicious IPs Identified: {len(threat_indicators['suspicious_ips'])}\")\n",
    "print(f\"  Malicious Files Detected: {len(threat_indicators['malicious_files'])}\")\n",
    "print(f\"  Attack Types Observed: {len(threat_indicators['attack_types'])}\")\n",
    "\n",
    "print(f\"\\nML Analysis Results:\")\n",
    "print(f\"  Topics Discovered: {n_topics}\")\n",
    "print(f\"  Clusters Identified: {n_clusters}\")\n",
    "print(f\"  Classification Accuracy: Available in classification report above\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "if len(threat_indicators['suspicious_ips']) > 0:\n",
    "    print(f\"  - Investigate suspicious IPs: {list(threat_indicators['suspicious_ips'])}\")\n",
    "if threat_indicators['attack_types']['brute_force'] > 2:\n",
    "    print(f\"  - Implement additional brute force protection\")\n",
    "if (logs_df['severity'] == 'critical').sum() > 0:\n",
    "    print(f\"  - Review {(logs_df['severity'] == 'critical').sum()} critical severity events immediately\")\n",
    "print(f\"  - Monitor clusters with high anomaly scores for new threat patterns\")\n",
    "\n",
    "print(\"\\n✅ NLP-based Security Log Analysis Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.12.3"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}