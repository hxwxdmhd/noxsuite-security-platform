#!/usr/bin/env python3
"""
NoxPanel Development Session Manager
===================================

Manages development sessions across the optimized workspace environment,
tracking performance, validating LSP behavior, and maintaining cache efficiency.

Author: NoxPanel Optimization Team
Date: July 19, 2025
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
import subprocess
import threading

class DevelopmentSessionManager:
    """Manages optimized development sessions with performance monitoring"""
    
    def __init__(self, workspace_root: str = r"k:\Project Heimnetz"):
        self.workspace_root = Path(workspace_root)
        self.session_log = self.workspace_root / "data" / "logs" / "dev_session.log"
        self.performance_log = self.workspace_root / "data" / "performance_tracking.json"
        self.current_workspace = None
        self.session_start = datetime.now()
        self.session_id = f"dev_session_{self.session_start.strftime('%Y%m%d_%H%M%S')}"
        
        # Ensure log directories exist
        self.session_log.parent.mkdir(exist_ok=True, parents=True)
        
        # Initialize session tracking
        self.session_data = {
            "session_id": self.session_id,
            "start_time": self.session_start.isoformat(),
            "workspace_switches": [],
            "performance_metrics": [],
            "lsp_validations": [],
            "cache_operations": [],
            "development_actions": []
        }
        
    def log_action(self, action_type: str, details: Dict[str, Any]):
        """Log development actions with timestamp"""
        timestamp = datetime.now().isoformat()
        log_entry = {
            "timestamp": timestamp,
            "action_type": action_type,
            "workspace": self.current_workspace,
            "details": details
        }
        
        self.session_data["development_actions"].append(log_entry)
        
        # Also log to file for persistent tracking
        with open(self.session_log, 'a') as f:
            f.write(f"{timestamp} | {action_type} | {self.current_workspace} | {json.dumps(details)}\n")
            
    def validate_workspace_performance(self, workspace_name: str) -> Dict[str, Any]:
        """Validate performance metrics for specific workspace"""
        workspace_file = self.workspace_root / f"{workspace_name}.code-workspace"
        
        if not workspace_file.exists():
            return {"status": "error", "message": f"Workspace {workspace_name} not found"}
        
        validation_results = {
            "workspace": workspace_name,
            "timestamp": datetime.now().isoformat(),
            "file_count": 0,
            "excluded_patterns": [],
            "lsp_settings": {},
            "cache_config": {},
            "performance_score": 0
        }
        
        try:
            # Load workspace configuration
            with open(workspace_file, 'r') as f:
                workspace_config = json.load(f)
            
            # Analyze folder structure and exclusions
            folders = workspace_config.get("folders", [])
            total_files = 0
            excluded_patterns = set()
            
            for folder in folders:
                folder_path = self.workspace_root / folder["path"].lstrip("./")
                if folder_path.exists():
                    files = list(folder_path.rglob("*.py"))
                    total_files += len(files)
                
                # Collect exclusion patterns
                if "exclude" in folder:
                    excluded_patterns.update(folder["exclude"])
            
            validation_results["file_count"] = total_files
            validation_results["excluded_patterns"] = list(excluded_patterns)
            
            # Extract LSP settings
            settings = workspace_config.get("settings", {})
            lsp_settings = {k: v for k, v in settings.items() if "python.analysis" in k or "typescript" in k}
            validation_results["lsp_settings"] = lsp_settings
            
            # Calculate performance score based on file count and optimizations
            base_score = max(0, 100 - (total_files / 20))  # Penalty for file count
            exclusion_bonus = len(excluded_patterns) * 5    # Bonus for exclusions
            lsp_bonus = len(lsp_settings) * 2               # Bonus for LSP optimization
            
            validation_results["performance_score"] = min(100, base_score + exclusion_bonus + lsp_bonus)
            
            # Log validation
            self.session_data["lsp_validations"].append(validation_results)
            self.log_action("workspace_validation", validation_results)
            
            return validation_results
            
        except Exception as e:
            error_result = {"status": "error", "message": str(e)}
            self.log_action("validation_error", error_result)
            return error_result
            
    def switch_workspace(self, workspace_name: str) -> bool:
        """Switch to specified workspace and validate performance"""
        try:
            # Validate workspace first
            validation = self.validate_workspace_performance(workspace_name)
            
            if validation.get("status") == "error":
                print(f"‚ùå Cannot switch to {workspace_name}: {validation.get('message')}")
                return False
            
            # Record workspace switch
            switch_record = {
                "from_workspace": self.current_workspace,
                "to_workspace": workspace_name,
                "timestamp": datetime.now().isoformat(),
                "file_count": validation["file_count"],
                "performance_score": validation["performance_score"]
            }
            
            self.session_data["workspace_switches"].append(switch_record)
            self.current_workspace = workspace_name
            
            print(f"üöÄ Switched to {workspace_name}")
            print(f"   üìÅ Target files: {validation['file_count']}")
            print(f"   üìä Performance score: {validation['performance_score']:.1f}/100")
            print(f"   üîß LSP optimizations: {len(validation['lsp_settings'])} active")
            
            self.log_action("workspace_switch", switch_record)
            return True
            
        except Exception as e:
            print(f"‚ùå Workspace switch failed: {e}")
            self.log_action("switch_error", {"workspace": workspace_name, "error": str(e)})
            return False
            
    def monitor_lsp_behavior(self) -> Dict[str, Any]:
        """Monitor LSP behavior and performance in current workspace"""
        if not self.current_workspace:
            return {"status": "error", "message": "No active workspace"}
        
        lsp_metrics = {
            "workspace": self.current_workspace,
            "timestamp": datetime.now().isoformat(),
            "python_lsp": {"status": "unknown", "response_time": 0, "memory_usage": 0},
            "typescript_lsp": {"status": "unknown", "response_time": 0, "memory_usage": 0},
            "overall_health": "unknown"
        }
        
        try:
            # Simulate LSP health check (in real implementation, would check actual LSP servers)
            workspace_file = self.workspace_root / f"{self.current_workspace}.code-workspace"
            
            if workspace_file.exists():
                with open(workspace_file, 'r') as f:
                    config = json.load(f)
                
                settings = config.get("settings", {})
                
                # Analyze Python LSP configuration
                python_settings = {k: v for k, v in settings.items() if "python" in k.lower()}
                if python_settings:
                    # Estimate performance based on configuration
                    memory_conservative = settings.get("python.analysis.memory.keepLibraryAst", True) is False
                    indexing_enabled = settings.get("python.analysis.indexing", True)
                    
                    lsp_metrics["python_lsp"] = {
                        "status": "active",
                        "response_time": 0.8 if memory_conservative else 2.5,
                        "memory_usage": 1200 if memory_conservative else 2800,
                        "indexing_enabled": indexing_enabled,
                        "memory_conservative": memory_conservative
                    }
                
                # Analyze TypeScript LSP configuration
                typescript_settings = {k: v for k, v in settings.items() if "typescript" in k.lower()}
                if typescript_settings:
                    auto_detect_off = settings.get("typescript.tsc.autoDetect", "on") == "off"
                    
                    lsp_metrics["typescript_lsp"] = {
                        "status": "active",
                        "response_time": 0.5 if auto_detect_off else 1.8,
                        "memory_usage": 800 if auto_detect_off else 1500,
                        "auto_detect_disabled": auto_detect_off
                    }
                
                # Calculate overall health score
                python_score = 100 - (lsp_metrics["python_lsp"]["response_time"] * 20)
                typescript_score = 100 - (lsp_metrics["typescript_lsp"]["response_time"] * 20)
                overall_score = (python_score + typescript_score) / 2
                
                if overall_score >= 80:
                    lsp_metrics["overall_health"] = "excellent"
                elif overall_score >= 60:
                    lsp_metrics["overall_health"] = "good"
                else:
                    lsp_metrics["overall_health"] = "needs_optimization"
                
                lsp_metrics["overall_score"] = overall_score
        
        except Exception as e:
            lsp_metrics["error"] = str(e)
            
        self.session_data["lsp_validations"].append(lsp_metrics)
        return lsp_metrics
        
    def validate_cache_efficiency(self) -> Dict[str, Any]:
        """Validate adaptive caching efficiency"""
        cache_analysis = {
            "timestamp": datetime.now().isoformat(),
            "workspace": self.current_workspace,
            "cache_directories": [],
            "cache_size": 0,
            "efficiency_score": 0,
            "recommendations": []
        }
        
        try:
            # Check workspace-specific cache
            if self.current_workspace:
                workspace_cache = self.workspace_root / ".vscode" / "adaptive_cache" / self.current_workspace
                if workspace_cache.exists():
                    cache_files = list(workspace_cache.glob("*.json"))
                    cache_size = sum(f.stat().st_size for f in cache_files)
                    
                    cache_analysis["cache_directories"].append(str(workspace_cache))
                    cache_analysis["cache_size"] = cache_size
                    cache_analysis["file_count"] = len(cache_files)
                    
                    # Calculate efficiency (more files cached = better efficiency)
                    if len(cache_files) > 0:
                        cache_analysis["efficiency_score"] = min(100, len(cache_files) * 2)
                        
                        if cache_size > 100 * 1024 * 1024:  # > 100MB
                            cache_analysis["recommendations"].append("Consider cache cleanup")
                        if len(cache_files) < 50:
                            cache_analysis["recommendations"].append("Cache warming may improve performance")
            
            # Check VS Code workspace cache
            vscode_cache = self.workspace_root / ".vscode"
            if vscode_cache.exists():
                settings_file = vscode_cache / "settings.json"
                if settings_file.exists():
                    cache_analysis["workspace_settings"] = "configured"
                    cache_analysis["efficiency_score"] += 20  # Bonus for configuration
                    
        except Exception as e:
            cache_analysis["error"] = str(e)
            
        self.session_data["cache_operations"].append(cache_analysis)
        return cache_analysis
        
    def get_recommended_workspace(self) -> str:
        """Get recommended workspace based on current context and performance"""
        workspace_scores = {}
        
        workspaces = ["NoxPanel-Core", "NoxPanel-AI", "NoxPanel-Plugins", "NoxPanel-DevOps"]
        
        for workspace in workspaces:
            validation = self.validate_workspace_performance(workspace)
            if "performance_score" in validation:
                workspace_scores[workspace] = validation["performance_score"]
        
        if workspace_scores:
            best_workspace = max(workspace_scores.items(), key=lambda x: x[1])
            return best_workspace[0]
        
        return "NoxPanel-Core"  # Default fallback
        
    def start_development_session(self, target_workspace: Optional[str] = None):
        """Start optimized development session"""
        print("üöÄ NoxPanel Development Session Manager")
        print("=" * 60)
        print(f"üìÖ Session ID: {self.session_id}")
        print(f"‚è∞ Start Time: {self.session_start.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # Determine target workspace
        if not target_workspace:
            target_workspace = self.get_recommended_workspace()
            print(f"üéØ Recommended workspace: {target_workspace}")
        
        # Switch to target workspace
        print("\nüîÑ Workspace Analysis:")
        if self.switch_workspace(target_workspace):
            print(f"‚úÖ Successfully switched to {target_workspace}")
            
            # Validate LSP behavior
            print("\nüß† LSP Performance Validation:")
            lsp_metrics = self.monitor_lsp_behavior()
            if "overall_health" in lsp_metrics:
                health = lsp_metrics["overall_health"]
                score = lsp_metrics.get("overall_score", 0)
                print(f"   üìä Overall LSP Health: {health} ({score:.1f}/100)")
                
                if "python_lsp" in lsp_metrics:
                    py_lsp = lsp_metrics["python_lsp"]
                    print(f"   üêç Python LSP: {py_lsp.get('response_time', 0):.1f}s response")
                    
                if "typescript_lsp" in lsp_metrics:
                    ts_lsp = lsp_metrics["typescript_lsp"]
                    print(f"   üìú TypeScript LSP: {ts_lsp.get('response_time', 0):.1f}s response")
            
            # Validate cache efficiency
            print("\nüíæ Cache Efficiency Analysis:")
            cache_metrics = self.validate_cache_efficiency()
            if "efficiency_score" in cache_metrics:
                score = cache_metrics["efficiency_score"]
                print(f"   üìà Cache Efficiency: {score}/100")
                
                if cache_metrics.get("recommendations"):
                    print("   üí° Recommendations:")
                    for rec in cache_metrics["recommendations"]:
                        print(f"      ‚Ä¢ {rec}")
            
            print(f"\nüéØ Development Environment Ready!")
            print(f"   üìÇ Workspace: {target_workspace}")
            print(f"   üìÅ Target Files: ~{self.validate_workspace_performance(target_workspace).get('file_count', 0)}")
            print(f"   ‚ö° Optimization: Active")
            print()
            
            return True
        else:
            print(f"‚ùå Failed to initialize workspace {target_workspace}")
            return False
            
    def save_session_data(self):
        """Save session data to performance log"""
        self.session_data["end_time"] = datetime.now().isoformat()
        self.session_data["duration"] = (datetime.now() - self.session_start).total_seconds()
        
        try:
            # Load existing performance data
            existing_data = []
            if self.performance_log.exists():
                with open(self.performance_log, 'r') as f:
                    existing_data = json.load(f)
            
            # Append current session
            existing_data.append(self.session_data)
            
            # Keep only last 50 sessions
            if len(existing_data) > 50:
                existing_data = existing_data[-50:]
            
            # Save updated data
            with open(self.performance_log, 'w') as f:
                json.dump(existing_data, f, indent=2, default=str)
                
            print(f"üìä Session data saved to {self.performance_log}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save session data: {e}")

def main():
    """Main development session entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description='NoxPanel Development Session Manager')
    parser.add_argument('command', choices=['start', 'validate', 'switch', 'status'], 
                       help='Command to execute')
    parser.add_argument('--workspace', 
                       choices=['NoxPanel-Core', 'NoxPanel-AI', 'NoxPanel-Plugins', 'NoxPanel-DevOps'],
                       help='Target workspace')
    
    args = parser.parse_args()
    
    manager = DevelopmentSessionManager()
    
    if args.command == 'start':
        success = manager.start_development_session(args.workspace)
        manager.save_session_data()
        sys.exit(0 if success else 1)
        
    elif args.command == 'validate':
        if args.workspace:
            validation = manager.validate_workspace_performance(args.workspace)
            print(json.dumps(validation, indent=2, default=str))
        else:
            print("Please specify --workspace for validation")
            
    elif args.command == 'switch':
        if args.workspace:
            success = manager.switch_workspace(args.workspace)
            sys.exit(0 if success else 1)
        else:
            print("Please specify --workspace to switch to")
            
    elif args.command == 'status':
        recommended = manager.get_recommended_workspace()
        print(f"Recommended workspace: {recommended}")
        
        # Show current performance metrics
        for ws in ['NoxPanel-Core', 'NoxPanel-AI', 'NoxPanel-Plugins', 'NoxPanel-DevOps']:
            validation = manager.validate_workspace_performance(ws)
            if 'performance_score' in validation:
                print(f"{ws}: {validation['performance_score']:.1f}/100 ({validation['file_count']} files)")

if __name__ == "__main__":
    main()
