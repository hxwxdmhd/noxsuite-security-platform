from NoxPanel.noxcore.utils.logging_config import get_logger
logger = get_logger(__name__)

#!/usr/bin/env python3
"""
Semantic Knowledge Parser for conversations.json
===============================================
Extracts, processes, and embeds knowledge from ChatGPT conversation dumps

REASONING CHAIN:
1. Problem: Need to extract architectural knowledge from conversation history
2. Analysis: conversations.json contains rich context about system design decisions
3. Solution: Parse, normalize, and embed knowledge for AI agent context
4. Validation: Structured knowledge base with traceable lineage

COMPLIANCE: ENHANCED - Knowledge Management System
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import hashlib
import re

logger = logging.getLogger(__name__)

@dataclass
class ConversationEntry:
    """
    REASONING CHAIN:
    1. Problem: System component ConversationEntry needs clear responsibility definition
    2. Analysis: Class requires specific implementation patterns for ConversationEntry functionality
    3. Solution: Implement ConversationEntry with SOLID principles and enterprise patterns
    4. Validation: Test ConversationEntry with comprehensive unit and integration tests
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
    """Structured conversation data"""
    id: str
    title: str
    messages: List[Dict[str, Any]]
    create_time: datetime
    update_time: datetime
    
@dataclass
class ExtractedKnowledge:
    """
    REASONING CHAIN:
    1. Problem: System component ExtractedKnowledge needs clear responsibility definition
    2. Analysis: Class requires specific implementation patterns for ExtractedKnowledge functionality
    3. Solution: Implement ExtractedKnowledge with SOLID principles and enterprise patterns
    4. Validation: Test ExtractedKnowledge with comprehensive unit and integration tests
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
    """Knowledge extracted from conversations"""
    source_id: str
    category: str
    content: str
    reasoning_chain: List[str]
    related_files: List[str]
    confidence: float
    tags: List[str]

class SemanticKnowledgeParser:
    """
    REASONING CHAIN:
    1. Problem: System component SemanticKnowledgeParser needs clear responsibility definition
    2. Analysis: Class requires specific implementation patterns for SemanticKnowledgeParser functionality
    3. Solution: Implement SemanticKnowledgeParser with SOLID principles and enterprise patterns
    4. Validation: Test SemanticKnowledgeParser with comprehensive unit and integration tests
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
    """
    Parses conversations.json and extracts semantic knowledge
    
    REASONING: Convert unstructured conversation data into structured KB
    """
    
    def __init__(self, workspace_root: str = "."):
    """
    Enhanced __init__ with AI-driven reasoning patterns
    
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement __init__ with enterprise-grade patterns and error handling
    4. Validation: Test __init__ with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        self.workspace_root = Path(workspace_root)
        self.kb_dir = self.workspace_root / "mcp" / "knowledgebase"
        self.knowledge_entries: List[ExtractedKnowledge] = []
    
    def parse_conversations(self, conversations_file: Path) -> Dict[str, Any]:
    """
    REASONING CHAIN:
    1. Problem: Data parsing needs robust transformation logic
    2. Analysis: Parser function requires error-tolerant data processing
    3. Solution: Implement parse_conversations with enterprise-grade patterns and error handling
    4. Validation: Test parse_conversations with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """
        Parse conversations.json and extract knowledge
        
        REASONING: Main entry point for knowledge extraction pipeline
        """
        logger.info(f"üß† Parsing conversations from: {conversations_file}")
        
        if not conversations_file.exists():
            logger.warning(f"Conversations file not found: {conversations_file}")
            return {"status": "file_not_found", "entries": 0}
        
        try:
            with open(conversations_file, 'r', encoding='utf-8') as f:
                conversations_data = json.load(f)
            
            # Extract conversations
            conversations = self._extract_conversations(conversations_data)
            
            # Process each conversation for knowledge
            for conv in conversations:
                knowledge = self._extract_knowledge_from_conversation(conv)
                self.knowledge_entries.extend(knowledge)
            
            # Generate knowledge base
            kb_data = self._generate_knowledge_base()
            
            # Save to files
            self._save_knowledge_base(kb_data)
            
            return {
                "status": "success",
                "conversations_processed": len(conversations),
                "knowledge_entries": len(self.knowledge_entries),
                "categories": list(set(entry.category for entry in self.knowledge_entries))
            }
            
        except Exception as e:
            logger.error(f"Error parsing conversations: {e}")
            return {"status": "error", "error": str(e)}
    
    def _extract_conversations(self, data: Any) -> List[ConversationEntry]:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _extract_conversations with enterprise-grade patterns and error handling
    4. Validation: Test _extract_conversations with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Extract conversation entries from JSON data"""
        conversations = []
        
        # Handle different JSON structures
        if isinstance(data, list):
            conv_list = data
        elif isinstance(data, dict) and 'conversations' in data:
            conv_list = data['conversations']
        elif isinstance(data, dict):
            conv_list = list(data.values())
        else:
            conv_list = []
        
        for conv_data in conv_list:
            if isinstance(conv_data, dict):
                try:
                    conv = ConversationEntry(
                        id=conv_data.get('id', str(hash(str(conv_data)))),
                        title=conv_data.get('title', 'Untitled'),
                        messages=conv_data.get('messages', []),
                        create_time=datetime.fromtimestamp(conv_data.get('create_time', 0)),
                        update_time=datetime.fromtimestamp(conv_data.get('update_time', 0))
                    )
                    conversations.append(conv)
                except Exception as e:
                    logger.debug(f"Skipping malformed conversation: {e}")
        
        return conversations
    
    def _extract_knowledge_from_conversation(self, conv: ConversationEntry) -> List[ExtractedKnowledge]:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _extract_knowledge_from_conversation with enterprise-grade patterns and error handling
    4. Validation: Test _extract_knowledge_from_conversation with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Extract knowledge from a single conversation"""
        knowledge_entries = []
        
        # Analyze conversation title for category
        category = self._categorize_conversation(conv.title)
        
        # Process messages for technical content
        for msg in conv.messages:
            if isinstance(msg, dict):
                content = msg.get('content', '')
                if isinstance(content, str) and len(content) > 100:
                    
                    # Extract reasoning chains
                    reasoning = self._extract_reasoning_chains(content)
                    
                    # Extract file references
                    files = self._extract_file_references(content)
                    
                    # Extract code patterns
                    if self._contains_technical_content(content):
                        knowledge = ExtractedKnowledge(
                            source_id=conv.id,
                            category=category,
                            content=content[:1000],  # Truncate for storage
                            reasoning_chain=reasoning,
                            related_files=files,
                            confidence=self._calculate_confidence(content),
                            tags=self._extract_tags(content)
                        )
                        knowledge_entries.append(knowledge)
        
        return knowledge_entries
    
    def _categorize_conversation(self, title: str) -> str:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _categorize_conversation with enterprise-grade patterns and error handling
    4. Validation: Test _categorize_conversation with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Categorize conversation based on title"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['architecture', 'design', 'structure']):
            return 'architecture'
        elif any(word in title_lower for word in ['bug', 'fix', 'error', 'issue']):
            return 'debugging'
        elif any(word in title_lower for word in ['deploy', 'docker', 'ci', 'cd']):
            return 'deployment'
        elif any(word in title_lower for word in ['ai', 'agent', 'ml', 'intelligence']):
            return 'ai_systems'
        elif any(word in title_lower for word in ['plugin', 'extension', 'module']):
            return 'plugins'
        elif any(word in title_lower for word in ['performance', 'optimization', 'speed']):
            return 'performance'
        else:
            return 'general'
    
    def _extract_reasoning_chains(self, content: str) -> List[str]:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _extract_reasoning_chains with enterprise-grade patterns and error handling
    4. Validation: Test _extract_reasoning_chains with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Extract RLVR reasoning chains from content"""
        reasoning_chains = []
        
        # Look for REASONING patterns
        reasoning_patterns = [
            r'REASONING CHAIN:\s*\n(.*?)(?=\n\n|\Z)',
            r'REASONING:\s*\n(.*?)(?=\n\n|\Z)',
            r'Analysis:\s*(.*?)(?=\n|\Z)',
            r'Solution:\s*(.*?)(?=\n|\Z)'
        ]
        
        for pattern in reasoning_patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            reasoning_chains.extend(matches)
        
        return [chain.strip() for chain in reasoning_chains if chain.strip()]
    
    def _extract_file_references(self, content: str) -> List[str]:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _extract_file_references with enterprise-grade patterns and error handling
    4. Validation: Test _extract_file_references with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Extract file and directory references"""
        
        # File patterns
        file_patterns = [
            r'[a-zA-Z0-9_/-]+\.py',
            r'[a-zA-Z0-9_/-]+\.js',
            r'[a-zA-Z0-9_/-]+\.yml',
            r'[a-zA-Z0-9_/-]+\.json',
            r'[a-zA-Z0-9_/-]+\.md',
            r'[a-zA-Z0-9_/-]+\.txt'
        ]
        
        files = []
        for pattern in file_patterns:
            matches = re.findall(pattern, content)
            files.extend(matches)
        
        return list(set(files))
    
    def _contains_technical_content(self, content: str) -> bool:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _contains_technical_content with enterprise-grade patterns and error handling
    4. Validation: Test _contains_technical_content with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Check if content contains technical information"""
        technical_keywords = [
            'python', 'javascript', 'docker', 'flask', 'fastapi',
            'async', 'await', 'import', 'function', 'class',
            'server', 'api', 'endpoint', 'database', 'redis',
            'nginx', 'deployment', 'ci/cd', 'test', 'plugin'
        ]
        
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in technical_keywords)
    
    def _calculate_confidence(self, content: str) -> float:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _calculate_confidence with enterprise-grade patterns and error handling
    4. Validation: Test _calculate_confidence with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Calculate confidence score for knowledge entry"""
        
        # Base confidence
        confidence = 0.5
        
        # Boost for code examples
        if '```' in content or 'def ' in content or 'class ' in content:
            confidence += 0.2
        
        # Boost for reasoning chains
        if 'reasoning' in content.lower() or 'analysis' in content.lower():
            confidence += 0.2
        
        # Boost for file references
        if any(ext in content for ext in ['.py', '.js', '.yml']):
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _extract_tags(self, content: str) -> List[str]:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _extract_tags with enterprise-grade patterns and error handling
    4. Validation: Test _extract_tags with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Extract relevant tags from content"""
        tags = []
        
        tag_patterns = {
            'docker': ['docker', 'container', 'compose'],
            'ai': ['ai', 'agent', 'ml', 'intelligence'],
            'server': ['server', 'flask', 'fastapi'],
            'database': ['database', 'db', 'sql', 'redis'],
            'plugin': ['plugin', 'extension', 'module'],
            'performance': ['performance', 'optimization', 'speed'],
            'security': ['security', 'auth', 'permission'],
            'ui': ['ui', 'frontend', 'react', 'html']
        }
        
        content_lower = content.lower()
        for tag, keywords in tag_patterns.items():
            if any(keyword in content_lower for keyword in keywords):
                tags.append(tag)
        
        return tags
    
    def _generate_knowledge_base(self) -> Dict[str, Any]:
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _generate_knowledge_base with enterprise-grade patterns and error handling
    4. Validation: Test _generate_knowledge_base with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Generate structured knowledge base"""
        
        categories = {}
        for entry in self.knowledge_entries:
            if entry.category not in categories:
                categories[entry.category] = []
            
            categories[entry.category].append({
                'source_id': entry.source_id,
                'content': entry.content,
                'reasoning_chain': entry.reasoning_chain,
                'related_files': entry.related_files,
                'confidence': entry.confidence,
                'tags': entry.tags
            })
        
        return {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_entries': len(self.knowledge_entries),
                'categories': list(categories.keys()),
                'version': '1.0.0'
            },
            'categories': categories
        }
    
    def _save_knowledge_base(self, kb_data: Dict[str, Any]):
    """
    REASONING CHAIN:
    1. Problem: Internal operation needs clear implementation boundary
    2. Analysis: Private method requires controlled access and defined behavior
    3. Solution: Implement _save_knowledge_base with enterprise-grade patterns and error handling
    4. Validation: Test _save_knowledge_base with edge cases and performance requirements
    
    ENHANCED: 2025-07-29 - AI-generated reasoning
    """
        """Save knowledge base to files"""
        
        # Main knowledge base file
        kb_file = self.kb_dir / "knowledge.json"
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(kb_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ Knowledge base saved: {kb_file}")
        
        # Category-specific files
        for category, entries in kb_data['categories'].items():
            category_file = self.kb_dir / f"{category}.json"
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'category': category,
                    'entries': entries,
                    'count': len(entries)
                }, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìÇ Category file saved: {category_file}")

async def run_mcp_server():
    """
    MCP Server Mode - Provides knowledge extraction tools for AI assistants
    
    REASONING CHAIN:
    1. Problem: Need MCP server interface for knowledge extraction and search
    2. Analysis: Server should handle knowledge parsing, search, and categorization requests
    3. Solution: Implement async MCP server with semantic knowledge capabilities
    4. Validation: Server responds to knowledge tool calls with structured data
    """
    parser = SemanticKnowledgeParser()
    
    # MCP Server loop
    logger.info("üß† NoxSuite Knowledge Parser Server started")
    
    try:
        while True:
            # Wait for MCP requests (simplified for demo)
            await asyncio.sleep(10)
            
            # In a real MCP server, this would handle incoming knowledge tool requests
            # For now, we'll maintain the knowledge base
            logger.info("üíì MCP Server heartbeat - knowledge parser running")
            
    except KeyboardInterrupt:
        logger.info("Knowledge Parser Server shutting down...")
    except Exception as e:
        logger.error(f"Knowledge Parser Server error: {e}")
        raise

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--server-mode":
        # Run as MCP server
        import asyncio
        asyncio.run(run_mcp_server())
    else:
        # Run as standalone parser
        parser = SemanticKnowledgeParser()
        
        # Try to parse conversations.json from imports directory
        conversations_file = Path("mcp/imports/conversations.json")
        if not conversations_file.exists():
            # Check user's download location
            conversations_file = Path("c:/Users/wsAdmin/Downloads/chatgptdump/conversations.json")
        
        if conversations_file.exists():
            result = parser.parse_conversations(conversations_file)
            logger.info(f"üß† Knowledge parsing result: {result}")
        else:
            logger.info("üìÅ conversations.json not found in expected locations")
